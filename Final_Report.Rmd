---
title: "Experimental_Methodology"
author: "Mark Paluta, Krysten Thompson, Chris Ventura"
date: "April 19, 2019"
output: 
  pdf_document:
    toc: true
    number_sections : true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE, warning=FALSE}
library(data.table)
library(corrplot)
library(stargazer)
library(pwr)
library(sandwich)
library(lmtest)
```

# Introduction / Background / Motivation
<TODO>
- Clean up, properly cite sources
- Clearly state research question and hypothesis
- Expand summary at the end (<> sections)

**Statement of the research question; why this is interesting to someone who is predisposed to be interested; what other people know about this; what (if any) obsrevational work has to say about this question**


< Background info on difficulties for older people finding housing >
- Fair housing act does not explicitly ban age discrimination in housing (https://www.justice.gov/crt/fair-housing-act-1)
- NYT article on rise of roommates for adults over 30 (https://www.nytimes.com/2016/05/06/fashion/mens-style/adult-men-roommates-new-york.html)
- Atlantic article on rise of roomates (https://www.theatlantic.com/family/archive/2018/08/the-strange-unique-intimacy-of-the-roommate-relationship/567296/)
- Zillow research article on doubling up (https://www.zillow.com/research/rising-rents-more-roommates-17618/)

In a recent study, Zillow found that the share of adults living with a non-spouse, non-partnered roomate increased from 21% in 2005 to 30% at the end of 2017.  Whether it is due to changing attitudes about living with roommates, a neccesity due to increased rents in major cities, or some other factor, the trend is clear: more Americans are cohabitating with someone other than a spouse or partner.  While college and post-college aged urbanites often expect to have roommates, the trend is increasing for those older than 30, as a recent New York Times profile depicts.  As employment opportunities continue to cluster in dense metropolitan areas such as New York and San Francisco, an older cohort may be forced to cohabitate in order to pursue economic opportunity.

In spite of this demographic trend, little research has been done into whether the age of someone looking for a place to live with roommates has an impact on their options. As the New York Times profile describes, until recently the idea of having a roommate was mostly relegated to young professionals in their 20s.  

# Research Question

Through this experiment, we set out to determine if older professionals face a penalty when seeking a roommate compared to someone in their 20's. We investigate whether age affects replies when responding to a roommate posting on Craigslist as well as determine whether age has a statistically significant effect on the odds of what we will call a "favorable reply".

We hypothesized that the "older" age treatment would receive fewer favorable replies than the 20-something treatment.

# Initial Experiment Idea

Our initial experiment sought to test gender and age discrimination. We created two email accounts for a female (age 27 and 43) and two email accounts for a male (age 27 and 43). We chose these ages to avoid round numbers, have the 20-something presumed to be slightly more established in a job/career than someone who is 22 or 23 (too young of an age may have caused bias), and have the older person at least over 40 but not yet 45 which may have caused bias as well. 

We also considered running this experiment in several different cities in the U.S. to see if regional differences exist in terms of gender and age.

A pilot was conducted in early March to determine if our initial approach would work and identify potential obstacles. We discovered issues with Craigslist and Gmail fraud mechanisms that led us to modify our approach. See Pilot Study below for further details. 

## Pilot Study and Outcomes

We conducted a pilot study for three days to identify any problems that may surface during the full study. Major learnings from the pilot:

1) VPNs are not particularly compatible with Craigslist. We initially wanted to use a VPN to avoid being flagged as fraud by Craigslist. We tried several but Craigslist still flagged us as fraud when it detected that we changed email accounts.

2) Google sent one of our accounts a notification that it was shutting down one of our accounts because it detected the account was not being used according to Google's guidelines. 

We concluded that maintaining too many personas would be difficult due to concerns about tripping fraud algorithms and getting our real IP addresses blacklisted. As a result, we decided to limit our full study to 2 females. We theorized that females would have higher response rates due to more postings seeming open to females, and suspected that males would be more willing to live with females than females would be willing to live with males.

Not all posts seemed appropriate for measuring effects on a typical applicant. The following types of posts were excluded:

  a) Posts with sexual implications including "friends with benefits", reduced rent in exchange for sex 
  b) Posts soliciting other favors in exchange for reduced rent such as babysitting
  c) Posts for students only
  d) Posts without an email option or posts explicitly stating that email replies will not be answered
  e) Posts explicitly stating males only


# Experiment Design & Methodology

The experiment design consisted of sending emails to listings under the Craigslist heading "Housing" > "Rooms/Shared". 

In order to be able to draw conclusions that could be somewhat generalized to the US population, Indianapolis was selected as the target market. Indianapolis is a mid-sized city located in the midwest.

We sent emails every 1-3 days across a two-week time period. We were not too concerned with the exact times or days we collected data since we were collecting a number of time-based covariates in order to not confound our results with any temporal effects. We did make a conscious effort to cover a broad range of variation in our temporal covariates by sending emails on every day of the week and including a mix of morning, afternoon, and evening collections.

The following procedure was implemented each time emails were sent:

1) Open all postings since the last data collection (or fewer if subject to time constraints, prioritizing recent postings)
2) Go through each post and throw out any that meet our exclusion criteria
3) Obtain a count of the number of included (remaining) posts
4) Randomize an array assigning these posts to our 27-year-old or 43-year-old persona.
5) Beginning with the first persona in the array, log into their respective email account and begin emailing posters in order from most recent to least, recording covariates as we go

Emails were sent only to listings that were new, or listings that had been posted days or weeks ago but were "refreshed" in the postings list. There were only 1-2 instances where a listing received more than one "Katie" email and those observations were deleted from the final data set.

A short R radomization function was run prior to sending out the emails to radomize which listing would receive a "Katie 27" or "Katie 43" email.

The following was collected by manually entering data in a Google Sheet:

- Listing post date/time
- Email sent to listing date/time
- Treatment (Katie 27, Katie 43)
- Listing title, description
- Age and gender of person posting listing (if avail)
- Whether reply was received, along with date/time
- Whether reply was favorable

Favorable replies were defined as those received within 72 hours of a "Katie" email being sent and suggesting a time to show the unit or asking "Katie" for more information. An unfavorable reply stated the unit was already rented. 

<TODO>

**Add screenshot of email**
- Add flow diagram
<Add diagram of response flows (emails/listing -> replies -> favorable replies)>
 **Clear statement of the experiment**

    Research Design (using ROXO grammar)
    Randomization engineering
    Experimental materials (e.g. treatment materials)
    Measurement of variables
    Modeling choices
**end**


# Data   

<TODO>

- Number of observations, how many for Katie_27, how many for Katie_43
- Talk about missing data for several variables like poster age and gender
- Don't show any code
- Include any EDA charts here that make sense

- Determine how we ant to present this.  Is this section necessary?  I think we can either just add a paragraph in methodology about our collection techniques or detail in EDA

```{r include=FALSE}
#df = read.csv("Data_Collection_Sheet.csv", fileEncoding="UTF-8-BOM")
d <- read.csv("craigslist.csv", stringsAsFactors = FALSE)
d$email_sent_date = NULL
d$email_sent_time = NULL
d$post_date = NULL
d$post_time = NULL
d$reply_date = NULL
d$reply_time = NULL
```

We collected a few types of information:

1) Title and body content of the post to assist in identifying duplicates and to archive this data in case the post is taken down.
```{r}
colnames(d)[3:4]
```

2) Post metadata
```{r}
colnames(d)[c(2,5)]
```

3) Covariates that may themselves predict high or low response rate
```{r}
colnames(d)[6:9]
```

4) Which treatment was applied
```{r}
colnames(d)[10]
```

5) Timestamp information for our response
```{r}
colnames(d)[1]
```

6) Reply (outcome) information
```{r}
colnames(d)[11:13]
```

<!-- 7) Additional covariate information determined from replies -->
```{r}
#colnames(d)[14,15]
```

# Exploratory Data Analysis

**Don't think we should include this section**

<TODO>
- Do some initial data descriptions
- Describe cleanup steps
- Create and describe visualizations

We will present some of the peculiarities of our data. First, we will need timestamps in a convenient format for analysis.

## Data Cleaning and Calculation

**I don't we should include this section**

#### Correct data formats
Timestamps
```{r timestamp transformations}
d$sent_timestamp <- as.POSIXct(d$sent_timestamp,format="%m/%d/%Y %H:%M", tz="EST")
d$post_timestamp <- as.POSIXct(d$post_timestamp,format="%m/%d/%Y %H:%M", tz="EST")
d$reply_timestamp <- as.POSIXct(d$reply_timestamp,format="%m/%d/%Y %H:%M", tz="EST")
```

Timestamp distances
```{r}
d$hours_post_to_email <- as.numeric(difftime(d$sent_timestamp,d$post_timestamp, units="hours"))
d$hours_email_to_reply <- as.numeric(difftime(d$reply_timestamp,d$sent_timestamp, units="hours")) # Note - not used will remove
```

Age
```{r}
d$poster_age = as.numeric(d$poster_age)
```

Combine list_types

```{r}
d <- within(d, list_type[list_type == 'townhouse'] <- 'house')
d <- within(d, list_type[list_type == 'condo'] <- 'apartment')
d$list_type[is.na(d$list_type)] <- 'unknown'
d$list_type = as.factor(d$list_type)
```

Create weekend variable

```{r}
d$sent_is_weekend <- ifelse(d$email_sent_dow %in% c(1,6,7), 1, 0)
```

Binarize Treatment

```{r}
d <- within(d, {
  fave_reply = ifelse(fave_reply == "Y", 1, 0)
})
```


< Provide some context around these outputs and decide which ones to include >
< Additional ideas:
  - Two bars, side by side, with probability of favorable replies and error margins
  - line chart showing probability of favorable reply based on sent time for treatment and control
>

```{r EDA}
# head(d) # don't include this - at least not for the columns of text
str(d)
dim(d)
colSums(is.na(d))
colnames(d)

hist(d$list_price) # maybe worth log transforming this
hist(log(d$list_price))

# create temporary dataframe casting categorical vars as 0-1 to allow corrplot
# d_temp <- within(d_sub, {
#   list_type = ifelse(list_type == "house", 1, 0)
#   treatment = ifelse(treatment == "katie_27", 1, 0)
#   #reply = ifelse(reply == "Y", 1, 0)
#   fave_reply = ifelse(fave_reply == "Y", 1, 0)
# })
# correlations = cor(na.omit(d_temp[, c('list_price', 'list_type', 'treatment', 'fave_reply', "mins_post_to_email")]))
# corrplot(correlations, method="color", type="upper", addCoef.col = "black",
#          col=colorRampPalette(c("#BB4444",  "#FFFFFF", "#4477AA"))(200))
```


# Results

### Regressions

To estimate our treatment effects, we employed a variety of regression specifications.  All of the regression specificaitons followed the following equations:

<Add equation in latex>

We used a linear regression for each of our four regression specifications, regressing the binary variable of a favorable reply on our treatment and, for the first three specifications, our chosen covariates.  While our outcome variable is binary, using a linear regression allows us to directly measure the expected change in probability of getting a favorable reply, with the coefficients of the independent variables corresponding to the probability increase or decrease.

**<Should we reverse this?  I feel like most papers I see usually have the reverse order (treatment only first, full gamut last)>**

The first regression regresses favorable reply on the treatment, the log of list price, binary indicators for house and unknown list types (leaving apartments as the default), the log of hours between the post time and email sent, and a binary indicator indicating if the treament or control email was sent on a weekend.  Given that we have only 113 observations, we were wary of including all of our variables for fear of overfitting our regressions and to preserve degrees of freedom.  A number of our variables were sparsely populated, so we selected these variables based on their near complete population for all observations as well as likelihood of explaining variations in a poster's response.  We chose to take the log of list price due to a skew in the values and the log of hours between post time and email sent for ease of interpretability

The second regression specification removes the variables associated with listing price and type, leaving only the treatment variable, the log of hours between post time and the email sent, and the binary indicator of the email being sent on the weekend.  This regression specification is meant to help ensure that we randomized effectively and that varibales related to the listing type were not correllated with the treatment.

The third regression leaves only treatment and the log of hours between post time and the email being sent.  We will detail this in the next section, but we wanted to ensure the only statistically significant covariate had no impact on the treatment effect.

Finally, the fourth regression contains only the treatment variable.

<remove summaries with working stargazer>
```{r}
# model 1: exhaustive regression
model.1 = lm(fave_reply ~ treatment+log(list_price)+list_type+log(hours_post_to_email)+sent_is_weekend, data=d)
model.1.robust <-coeftest(model.1, vcov = vcovHC(model.1, type="HC"))
model.1.robust
cov1 <- vcovHC(model.1, type = "HC")
robust.se.1 <- sqrt(diag(cov1))

# model 2: removing list type and price (rationale - maybe correlated with weekend sent)
model.2 = lm(fave_reply ~ treatment+log(hours_post_to_email)+sent_is_weekend, data=d)
model.2.robust <-coeftest(model.2, vcov = vcovHC(model.2, type="HC"))
model.2.robust
cov2 <- vcovHC(model.2, type = "HC")
robust.se.2 <- sqrt(diag(cov2))

# model 3: only treatment and hours_post_to_email (the only significant covariate)
model.3 = lm(fave_reply ~ treatment+log(hours_post_to_email), data=d)
model.3.robust <- coeftest(model.3, vcov = vcovHC(model.3, type="HC"))
model.3.robust
cov3 <- vcovHC(model.3, type = "HC")
robust.se.3 <- sqrt(diag(cov3))

# model 4: treatment only
model.4 = lm(fave_reply ~ treatment, data=d)
model.4.robust <- coeftest(model.4, vcov = vcovHC(model.4, type="HC"))
model.4.robust
cov4 <- vcovHC(model.4, type = "HC")
robust.se.4 <- sqrt(diag(cov4))

#plot(model.1)
#plot(model.2)
#plot(model.3)
#plot(model.4)
```


``` {r stargazer, results='asis'}
stargazer(model.1, model.2, model.3, model.4, se=c(robust.se.1, robust.se.2, robust.se.3, robust.se.4))
```

Examining the results of the four regression specifications, we see a fairly consistent treatment effect ranging from `r round(model.1.robust['treatmentkatie_43','Estimate'], 2)` to `r round(model.4.robust['treatmentkatie_43','Estimate'], 2)`.  This suggests that our randomization worked appropriately, and that the effect of being a 43 year old female as opposed to a 27 year old females was to reduce the probability of receiving a response to roommate listing inquiries by `r abs(round(model.4.robust['treatmentkatie_43','Estimate'], 2))` to `r abs(round(model.1.robust['treatmentkatie_43','Estimate'], 2))`.  In spite of this practical significance, however, this result lacked statistical signifiance for all four model specifications with p-values ranging from `r round(model.1.robust['treatmentkatie_43','Pr(>|t|)'], 2)` to `r round(model.4.robust['treatmentkatie_43','Pr(>|t|)'], 2)`.

**<Please check my interpretation of the log values>**
In fact, the only variable to show any statistical significance in our model specifications is the log of hours between the post time and the sending of our treatment or control email. The coefficients of `r round(model.1.robust['log(hours_post_to_email)','Estimate'], 2)` to `r round(model.3.robust['log(hours_post_to_email)','Estimate'], 2)` suggests that a 100% increase in response time corresponds with a `r round(model.1.robust['log(hours_post_to_email)','Estimate'], 2)*100`% to `r round(model.3.robust['log(hours_post_to_email)','Estimate'], 2)*100`% change in likelihood of a response.


## Two-proportion z-test

The second way we will analyze this data is simply a two-proportion z-test.

<TODO>
- Code!
- Interpret results



## Randomization Inference

```{r}
d_dt <- data.table(d)

reply_treatment <- d_dt[treatment=="katie_43", mean(fave_reply==1)]
reply_base <- d_dt[treatment=="katie_27", mean(fave_reply==1)]
ate <- reply_treatment - reply_base
print(ate)
```


```{r}
iter <- 10000
ate_vec <- rep(NA, length(iter))
for(i in 1:iter){
d_dt[, success_rand := sample(c(0,1), size = .N, replace=TRUE)]
ate_vec[i] <- as.numeric(d_dt[ , .('success_mean' = mean(fave_reply==1)), keyby =.(success_rand)][ , .('ate' = diff(success_mean, lag=1, differences=1))])
}
```

```{r}
ri <- table(ate_vec<ate)
ri
```

In addition, we also used randomization inference to examine the probability of calculating our treatment effect by chance.  Examining the difference in means, we calculate an average treatment effect of `r round(ate,4)`.  Running randomization inference with `r iter` iteractions, we find `r as.numeric(ri[2])` random iterations with at least as significant a treatment effect as our actual.  This implies a p-value of `r as.numeric(ri[2]) / as.numeric(iter)`, a non-statistically significant result.  This is consistent with our linear regression results showing a non-statistically significant treatment effect.

## Power Calculation

```{r}
power_n <- pwr.t.test(d=0.022, sig.level=0.05, power=.8)
power_d <- pwr.t.test(n=113, sig.level=0.05, power=.8)
```
<TODO>
- Add narrative



# Conclusions
<TODO>
-write conclusion lol
**Clear statement of results**
    In text description of your results
    Figures and tables that support your in text description
    Clean, clear, well articulated relationships between your theory, your hypotheses, the numbers that your models produce, and the figures you present.
**end**
